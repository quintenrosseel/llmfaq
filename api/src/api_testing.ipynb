{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing The API\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from fastapi import FastAPI\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA  # Q&A retrieval system.\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter\n",
    "import uvicorn\n",
    "import os\n",
    "import langchain\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA MODEL =============================================\n",
    "class FeedbackEnum(str, Enum):\n",
    "    \"\"\" \n",
    "        A Pydantic model representing feedback. \n",
    "    \"\"\"\n",
    "    like = 1\n",
    "    neutral = 0\n",
    "    dislike = -1\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model representing a LLM prompt\n",
    "    \"\"\"\n",
    "    prompt: str\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model representing a search question. \n",
    "    \"\"\"\n",
    "    search_string: str\n",
    "\n",
    "class ChunkMetadata(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model representing metadata.\n",
    "    \"\"\"\n",
    "    chunk_size: int\n",
    "    embedding_model: str\n",
    "    chunk_order: int\n",
    "    chunk_overlap: int\n",
    "    chunk_id: int\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model representing a document.\n",
    "    Wrapper around langchains langchain.schema.document.Document.\n",
    "    \"\"\"\n",
    "    page_content: str\n",
    "    metadata: ChunkMetadata\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model representing an answer to a question.\n",
    "    \"\"\"\n",
    "    context: List[Document]\n",
    "    llm_prompt: str\n",
    "    llm_answer: str\n",
    "    language: str\n",
    "    score: FeedbackEnum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT_TEMPLATE = \"\"\"\n",
    "ANSWER THE QUESTION BASED ONLY ON THE FOLLOWING CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: \n",
    "{question}\n",
    "\n",
    "ANSWER IN THE FOLLOWING LANGUAGE: \n",
    "{language}\n",
    "\n",
    "CLEARLY STATE IF THE ANSWER CANNOT BE FOUND IN THE CONTEXT ABOVE.\n",
    "IF THE ANSWER CAN BE FOUND, REFERENCE THE CONTEXT. \n",
    "\"\"\"\n",
    "# Define the configuration\n",
    "RETRIEVER_SEARCH_CONFIG = {\n",
    "    # \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\".\n",
    "    'search_type': 'similarity', \n",
    "    'search_kwargs': {\n",
    "        # Amount of documents to return (default: 4).\n",
    "        'k': 5, \n",
    "        # Amount of documents to pass to the MMR algorithm \n",
    "        # # (default: 20).\n",
    "        'fetch_k': 50, \n",
    "        # Minimum relevance threshold for similarity_score_threshold.\n",
    "        'score_threshold': 0, \n",
    "        # Diversity of results returned by MMR; \n",
    "        # # 1 for minimum diversity and 0 for maximum (default: 0.5).\n",
    "        'lambda_mult': 0.25, \n",
    "        # Filter by document metadata.\n",
    "        'filter': {'chunk_size': 500}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "#### GLOBAL VARIABLES =================\n",
    "langchain.verbose = False\n",
    "langchain.debug = False\n",
    "load_dotenv() \n",
    "\n",
    "# Instructor Embeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-xl\", \n",
    "    cache_folder='./models'\n",
    ")\n",
    "\n",
    "# Graph from existing graph\n",
    "neo4j_graph = Neo4jVector.from_existing_index(\n",
    "    embedding=embeddings,\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USER\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name=\"vi_chunk_embedding_cosine\",\n",
    "    keyword_index_name=\"fts_Chunk_text\",\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "\n",
    "# Neo4j driver\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\"), \n",
    "    auth=(\n",
    "        os.getenv(\"NEO4J_USER\"), \n",
    "        os.getenv(\"NEO4J_PASSWORD\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Langchain retriever\n",
    "neo4j_retriever = neo4j_graph.as_retriever(**RETRIEVER_SEARCH_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RETRIEVE DOCUMENTS =================\n",
    "# Get docs from Neo4j\n",
    "docs: List[langchain.schema.document.Document] = (\n",
    "    neo4j_retriever.get_relevant_documents(\n",
    "        question\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Retriever Context \n",
    "context: List[Document] = [\n",
    "    Document(\n",
    "        page_content=doc.page_content, \n",
    "        metadata=ChunkMetadata(\n",
    "            chunk_id=doc.metadata['chunk_id'],\n",
    "            chunk_size=doc.metadata['chunk_size'],\n",
    "            embedding_model=doc.metadata['embedding_model'],\n",
    "            chunk_order=doc.metadata['chunk_order'],\n",
    "            chunk_overlap=doc.metadata['chunk_overlap']\n",
    "        )\n",
    "    ) for doc in docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='## Wat is een glucosesensor?' metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 1544, 'chunk_overlap': 60, 'chunk_id': 1544}\n",
      "page_content='## Wat is een circumcisie?' metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 320, 'chunk_overlap': 60, 'chunk_id': 320}\n",
      "page_content='## Wat is een facetinfiltratie?' metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 1394, 'chunk_overlap': 60, 'chunk_id': 1394}\n",
      "page_content='## Wat is een ERCP?' metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 1326, 'chunk_overlap': 60, 'chunk_id': 1326}\n",
      "page_content='## Wat is een fistel?' metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 1414, 'chunk_overlap': 60, 'chunk_id': 1414}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_str(l: List[langchain.schema.document.Document]) -> str:\n",
    "    page_content: str = \"\"\n",
    "    for i, d in enumerate(l): \n",
    "        # Page Content\n",
    "        page_content += (\n",
    "            (25 * \"=\") + \n",
    "            (f\" Document {i+1} \") + \n",
    "            (25 * \"=\") + \n",
    "            '\\n'\n",
    "        )\n",
    "        page_content += (\n",
    "            # (25 * \"=\" ) + \n",
    "            (f\"# Document {i+1} Metadata \") + \n",
    "            # (5 * \"=\" ) + \n",
    "            '\\n'\n",
    "        )\n",
    "        # Metadata \n",
    "        for k, v in d.metadata.items():\n",
    "            page_content += (f\"- {k}: {v} \\n\")\n",
    "        \n",
    "        page_content += (\n",
    "            # (25 * \"=\" ) + \n",
    "            (f\"# Document {i+1}'s Content\") + \n",
    "            # (5 * \"=\" ) + \n",
    "            '\\n'\n",
    "        )\n",
    "        page_content += d.page_content\n",
    "        page_content += '\\n'\n",
    "    return page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_paths_to_docs(chunk_paths: List[object]) -> List[Document]:\n",
    "    \"\"\"\n",
    "        For now, this matches chunk_paths of type (Chunk)-rel-(WebPage)-()\n",
    "    \"\"\"\n",
    "    # This returns paths, that we can turn into LangChain documents somehow. \n",
    "    path_docs: List[Document] = []\n",
    "\n",
    "    # One result for every chunk (see above)\n",
    "    for p in chunk_paths:\n",
    "        chunk_path_str = '' \n",
    "        chunk_node = p['rel'][0] \n",
    "        chunk_text = chunk_node.get('text')\n",
    "        \n",
    "        # Build up metadata of Document object manually\n",
    "        doc_meta = {\n",
    "            'chunk_size': chunk_node.get('chunk_size'),\n",
    "            'embedding_model': chunk_node.get('embedding_model'),\n",
    "            'chunk_order': chunk_node.get('chunk_order'),\n",
    "            'chunk_overlap': chunk_node.get('chunk_overlap'),\n",
    "            'chunk_id': chunk_node.get('chunk_id'),\n",
    "        }\n",
    "        # Traverse path for metadata\n",
    "        for i, o in enumerate(p['rel']): \n",
    "            # Create path representation\n",
    "            if type(o) == dict:\n",
    "                chunk_path_str += f'(Node)'\n",
    "            elif type(o) == str:\n",
    "                chunk_path_str += f'<-{o}-'\n",
    "            \n",
    "            # WebPage node data\n",
    "            if i == 2:\n",
    "                doc_meta['webpage_scrape_dt'] = o.get('scrape_dt')\n",
    "                doc_meta['webpage_url'] = o.get('url')\n",
    "                doc_meta['webpage_title'] = o.get('title')\n",
    "            # Catalog node data\n",
    "            elif i == 4: \n",
    "                doc_meta['catalog_url'] = o.get('url')\n",
    "        \n",
    "        # Add path structure as metadata\n",
    "        doc_meta['path_context'] = chunk_path_str   \n",
    "        \n",
    "        # Extract metadata from traversal \n",
    "        path_docs.append(\n",
    "            Document(\n",
    "                page_content=chunk_text, \n",
    "                metadata=doc_meta\n",
    "            )\n",
    "        )\n",
    "    return path_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neo4j_docs(question: str) -> List[langchain.schema.document.Document]:): \n",
    "    vec_chunk_paths = neo4j_graph.query(\n",
    "        f\"\"\"\n",
    "            CALL db.index.vector.queryNodes(\n",
    "                \"vi_chunk_embedding_cosine\", \n",
    "                {RESULT_LIM},\n",
    "                {embeddings.embed_query(VECTOR_SEARCH_QUERY)}\n",
    "            ) \n",
    "            YIELD node, score\n",
    "            WITH node, score\n",
    "            ORDER BY score DESCENDING\n",
    "            MATCH rel=(node:Chunk)<-[r*]-(:Catalog)\n",
    "            RETURN rel\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='## Wat moet je doen?\\n\\nOm zoveel mogelijk informatie uit het onderzoek te krijgen, hebben we jouw medewerking nodig. Hierop moet je letten:', metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 12, 'chunk_overlap': 60, 'chunk_id': 12}),\n",
       " Document(page_content='## Hoe bereid ik me het best voor?\\n\\n\\nJe hoeft niet nuchter te zijn. Voor de start van het onderzoek bespreken we met jou de taken die je tijdens het onderzoek moet uitvoeren. Dit kunnen motorische taken zijn (waarbij je een bepaald lichaamsdeel moet bewegen) of cognitieve taken (waarbij je alleen moet denken) of beiden. Zorg ervoor dat je de instructies goed begrijpt en dat je weet wat er van jou verwacht wordt tijdens het onderzoek. Dit is nodig om het onderzoek te doen slagen.', metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 1462, 'chunk_overlap': 60, 'chunk_id': 1462}),\n",
       " Document(page_content='* Bedek je neus en mond bij hoesten of niezen met een papieren zakdoek. Gooi de zakdoek onmiddellijk weg in een vuilnisemmer en was je handen.', metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 968, 'chunk_overlap': 60, 'chunk_id': 968}),\n",
       " Document(page_content='Er zijn verschillende types nierstenen (zes om precies te zijn), met elk hun eigen oorzaak en ontstaansmechanisme. Als je het type steen en de risicofactoren kent, kun je behandeling daarop afstemmen en is advies op maat mogelijk. \\n\\n\\nHeel concreet geeft het metabool bilan een antwoord op de volgende vragen:\\n\\n\\n* Waarom maak ik nierstenen aan?\\n* Welke maatregelen zijn nodig om mijn kans op herval te verminderen.\\n\\n\\n\\n## Voor wie?', metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 2431, 'chunk_overlap': 60, 'chunk_id': 2431}),\n",
       " Document(page_content='die je advies kan geven over voeding, levensstijl, etc.', metadata={'chunk_size': 500, 'embedding_model': 'hkunlp/instructor-xl', 'chunk_order': 2882, 'chunk_overlap': 60, 'chunk_id': 2882})]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo4j_retriever.get_relevant_documents(\n",
    "   \"Wat moet ik doen bij nierstenen?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "JE BENT EEN ZIEKENHUISADMINISTRATEUR.\n",
      "BEANTWOORD DE VRAAG DIE VOLGT ALLEEN OP BASIS VAN DE ONDERSTAANDE CONTEXT:\n",
      "========================= Document 1 =========================\n",
      "# Document 1 Metadata \n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 1462 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 1462 \n",
      "# Document 1's Content\n",
      "## Hoe bereid ik me het best voor?\n",
      "\n",
      "\n",
      "Je hoeft niet nuchter te zijn. Voor de start van het onderzoek bespreken we met jou de taken die je tijdens het onderzoek moet uitvoeren. Dit kunnen motorische taken zijn (waarbij je een bepaald lichaamsdeel moet bewegen) of cognitieve taken (waarbij je alleen moet denken) of beiden. Zorg ervoor dat je de instructies goed begrijpt en dat je weet wat er van jou verwacht wordt tijdens het onderzoek. Dit is nodig om het onderzoek te doen slagen.\n",
      "========================= Document 2 =========================\n",
      "# Document 2 Metadata \n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 12 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 12 \n",
      "# Document 2's Content\n",
      "## Wat moet je doen?\n",
      "\n",
      "Om zoveel mogelijk informatie uit het onderzoek te krijgen, hebben we jouw medewerking nodig. Hierop moet je letten:\n",
      "========================= Document 3 =========================\n",
      "# Document 3 Metadata \n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 2882 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 2882 \n",
      "# Document 3's Content\n",
      "die je advies kan geven over voeding, levensstijl, etc.\n",
      "========================= Document 4 =========================\n",
      "# Document 4 Metadata \n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 3536 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 3536 \n",
      "# Document 4's Content\n",
      "## Hoe verloopt een behandeling?\n",
      "========================= Document 5 =========================\n",
      "# Document 5 Metadata \n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 69 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 69 \n",
      "# Document 5's Content\n",
      "te geven, zodat je jouw dagelijkste activiteiten kan hervatten.\n",
      "\n",
      "\n",
      "VRAAG: \n",
      "Wat moet ik doen met\n",
      "\n",
      "INSTRUCTIES BIJ HET ANTWOORDEN:\n",
      "- ALGEMEEN:\n",
      "    - GEEF DUIDELIJK AAN ALS HET ANTWOORD NIET GEVONDEN KAN WORDEN IN DE DATABASE\n",
      "    - ANTWOORD ALTIJD IN HET Nederlands\n",
      "- INDIEN JE NIETS VINDT IN DE CONTEXT, VERMELD DAN:\n",
      "    - LIJST DE MEEST GELIJKE CONTEXT OP MET KOMMA'S\n",
      "    - DE URL VAN DE CONTEXT\n",
      "- INDIEN JE WEL IETS VINDT IN DE CONTEXT, VERMELD DAN:\n",
      "    - LIJST DE RELEVANTE CONTEXT OP MET KOMMA'S\n",
      "    - DE BRON (URL) VAN DE CONTEXT.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Op basis van de gegeven context kunnen we niet bepalen wat er moet gebeuren met\n",
      "de instructies.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "BASE_PROMPT_TEMPLATE_NL = \"\"\"\n",
    "JE BENT EEN ZIEKENHUISADMINISTRATEUR.\n",
    "BEANTWOORD DE VRAAG DIE VOLGT ALLEEN OP BASIS VAN DE ONDERSTAANDE CONTEXT:\n",
    "{context}\n",
    "\n",
    "VRAAG: \n",
    "{question}\n",
    "\n",
    "INSTRUCTIES BIJ HET ANTWOORDEN:\n",
    "- ALGEMEEN:\n",
    "    - GEEF DUIDELIJK AAN ALS HET ANTWOORD NIET GEVONDEN KAN WORDEN IN DE DATABASE\n",
    "    - ANTWOORD ALTIJD IN HET {language}\n",
    "- INDIEN JE NIETS VINDT IN DE CONTEXT, VERMELD DAN:\n",
    "    - LIJST DE MEEST GELIJKE CONTEXT OP MET KOMMA'S\n",
    "    - DE URL VAN DE CONTEXT\n",
    "- INDIEN JE WEL IETS VINDT IN DE CONTEXT, VERMELD DAN:\n",
    "    - LIJST DE RELEVANTE CONTEXT OP MET KOMMA'S\n",
    "    - DE BRON (URL) VAN DE CONTEXT.\n",
    "\"\"\"\n",
    "\n",
    "# QUESTION =============================================\n",
    "question = \"Wat moet ik doen met\"\n",
    "\n",
    "## RETRIEVE DOCUMENTS =================\n",
    "# Get docs from Neo4j\n",
    "docs: List[langchain.schema.document.Document] = (\n",
    "    neo4j_retriever.get_relevant_documents(\n",
    "        question\n",
    "    )\n",
    ")\n",
    "\n",
    "llm_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"language\"],\n",
    "    # partial_variables={\"Customer_Name\", \"Customer_State\", \"Customer_Gender\"},\n",
    "    template=BASE_PROMPT_TEMPLATE_NL,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=1)\n",
    "llm_chain = LLMChain(\n",
    "    prompt=llm_prompt, \n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm_answer = llm_chain.invoke(\n",
    "    input={\n",
    "        \"question\": question,\n",
    "        \"context\": docs_to_str(docs), \n",
    "        \"language\": \"Nederlands\"\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    "    include_run_info=False\n",
    ")\n",
    "\n",
    "# Show output\n",
    "# wrapped_context = textwrap.wrap(llm_prompt.)\n",
    "\n",
    "# Show answer \n",
    "wrapped_output = textwrap.wrap(llm_answer['text'], width=80)\n",
    "for line in wrapped_output:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Document 1 =========================\n",
      "=============== Document 1 Metadata ===============\n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 2431 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 2431 \n",
      "=============== Document 1's Content===============\n",
      "Er zijn verschillende types nierstenen (zes om precies te zijn), met elk hun eigen oorzaak en ontstaansmechanisme. Als je het type steen en de risicofactoren kent, kun je behandeling daarop afstemmen en is advies op maat mogelijk. \n",
      "\n",
      "\n",
      "Heel concreet geeft het metabool bilan een antwoord op de volgende vragen:\n",
      "\n",
      "\n",
      "* Waarom maak ik nierstenen aan?\n",
      "* Welke maatregelen zijn nodig om mijn kans op herval te verminderen.\n",
      "\n",
      "\n",
      "\n",
      "## Voor wie?\n",
      "========================= Document 2 =========================\n",
      "=============== Document 2 Metadata ===============\n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 3862 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 3862 \n",
      "=============== Document 2's Content===============\n",
      "## Hoe verloopt de opstart?\n",
      "\n",
      "Er wordt steeds gestart met een opleiding in het ziekenhuis. Patiënten en soms ook hun familie of mantelzorgers, wordt aangeleerd hoe ze zich moeten aansluiten op het toestel en hoe het toestel moet worden bediend. Bovendien wordt er getraind op alarmen: wat kan ik zelf bijsturen, wanneer moet ik het ziekenhuis bellen? Het moeilijkste aspect is het leren om zelf de fistel aan te prikken. We geven de patiënt hiervoor alle tijd die hij nodig heeft.\n",
      "========================= Document 3 =========================\n",
      "=============== Document 3 Metadata ===============\n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 729 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 729 \n",
      "=============== Document 3's Content===============\n",
      "De dagen na het onderzoek kan je stoelgang wit kleuren. Drink na het onderzoek voldoende water. Heb je toch last van constipatie, dan kan de aanvragende arts eventueel een laxeermiddel voorschrijven.\n",
      "\n",
      "\n",
      "\n",
      "## Hoe maak ik een afspraak?\n",
      "========================= Document 4 =========================\n",
      "=============== Document 4 Metadata ===============\n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 56 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 56 \n",
      "=============== Document 4's Content===============\n",
      "* De pijn neemt toe bij inspanning. Als je de buikspieren aanspant, veroorzaakt dit meestal een toename van de pijn. Liggen op de pijnlijke zijde kan ook voor toename van de pijn zorgen. Er kan een uitstraling naar de zij en de rug bestaan.\n",
      "* Je hebt een doffe, zeurende en soms stekende pijn. Afknelling van de gevoelszenuwtakjes kan leiden tot overgevoeligheid van het stukje buikhuid dat bij deze zenuw hoort.\n",
      "* Problemen met de ontlasting en/of (volle) blaas.\n",
      "========================= Document 5 =========================\n",
      "=============== Document 5 Metadata ===============\n",
      "- chunk_size: 500 \n",
      "- embedding_model: hkunlp/instructor-xl \n",
      "- chunk_order: 968 \n",
      "- chunk_overlap: 60 \n",
      "- chunk_id: 968 \n",
      "=============== Document 5's Content===============\n",
      "* Bedek je neus en mond bij hoesten of niezen met een papieren zakdoek. Gooi de zakdoek onmiddellijk weg in een vuilnisemmer en was je handen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs_to_str(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
